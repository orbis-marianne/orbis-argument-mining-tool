{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68f5060-7d22-49d7-b981-8c57cba65a71",
   "metadata": {},
   "source": [
    "# Statements Dataset Building\n",
    "\n",
    "In this notebook I'll be building what I call the \"Statements Dataset\". This is the view used by BCause, where the main idea is to understand whether a statement is either a position over a debate or an argument in favor or against a position. It treats arguments as a whole (instead of treating them at low level like the component detection and relation between components we traditionally use in argumentation mining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325d9a9-d957-4ecf-a9da-4db5ec864210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f55aff-1985-4706-8d3f-e4c25c86e2b9",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47b873-6769-4537-9934-1bc9dc521bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ad21a-8a93-4b63-92f8-b3a92f6a95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcause = []\n",
    "with open(DATA_DIR / \"bcause\" / \"bcause-data.jl\", \"rt\") as fh:\n",
    "    for line in fh:\n",
    "        bcause.append(json.loads(line))\n",
    "\n",
    "touche = []\n",
    "with open(DATA_DIR / \"touche23-valueeval\" / \"touche23-data.jl\", \"rt\") as fh:\n",
    "    for line in fh:\n",
    "        touche.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa70d190-f2eb-4b7b-938d-b784b5a21ce9",
   "metadata": {},
   "source": [
    "# Dataset for Statement Type Classification\n",
    "\n",
    "We build a dataset to detect whether a statement is a position or an attack/support type of argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1141e-9060-49fc-8a65-80d3b862bfd2",
   "metadata": {},
   "source": [
    "## Bcause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875dec3-9182-4208-9564-ee64f658d7bf",
   "metadata": {},
   "source": [
    "Bcause is the type of data expected, but it's also the more noisy, thus we use half of it for training and half for development (i.e., validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6046d9-9251-46d0-aed4-be79db67c9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcause_argument_classification = []\n",
    "\n",
    "for instance in tqdm(bcause):\n",
    "    bcause_argument_classification.append(\n",
    "        {\n",
    "            \"statement\": instance[\"text\"],\n",
    "            \"label\": f'__label__{instance[\"metadata\"][\"type\"]}',  # Add the __label__ prefix to keep it standard\n",
    "        }\n",
    "    )\n",
    "\n",
    "bcause_argument_classification = pd.DataFrame(bcause_argument_classification)\n",
    "bcause_argument_classification[\"split\"] = \"train\"\n",
    "\n",
    "val_index = bcause_argument_classification.sample(frac=0.5, random_state=42).index\n",
    "\n",
    "bcause_argument_classification.loc[val_index, \"split\"] = \"validation\"\n",
    "bcause_argument_classification.groupby([\"split\", \"label\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37f222-0d89-44b6-a80c-2c0d62022ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in bcause_argument_classification[\"split\"].unique():\n",
    "    bcause_argument_classification.loc[\n",
    "        bcause_argument_classification[\"split\"] == split, [\"label\", \"statement\"]\n",
    "    ].to_csv(\n",
    "        DATA_DIR / \"statements\" / \"classification\" / f\"bcause-{split}.tsv\",\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf5324-3c96-40c6-a277-296fb9908bef",
   "metadata": {},
   "source": [
    "## Touche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353cfcf4-2723-42db-8335-e0a0fe8601e3",
   "metadata": {},
   "source": [
    "For the Touche, the arguments already have an split (train, validation, test), but the conclusions (as they are part of multiple arguments) don't, thus we only make the split over them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9bee27-151b-44d6-8eb9-fc7c811531c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "touche_argument_classification = []\n",
    "\n",
    "for instance in tqdm(touche):\n",
    "    touche_argument_classification.append(\n",
    "        {\n",
    "            \"statement\": instance[\"text\"],\n",
    "            \"label\": f'__label__{instance[\"metadata\"][\"type\"]}',  # Add the __label__ prefix to keep it standard\n",
    "            \"split\": instance[\"metadata\"].get(\"original_split\", \"n/a\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "touche_argument_classification = pd.DataFrame(touche_argument_classification)\n",
    "non_splitted = touche_argument_classification[\n",
    "    touche_argument_classification[\"split\"] == \"n/a\"\n",
    "].index.values\n",
    "\n",
    "train_fraction = touche_argument_classification[touche_argument_classification[\"split\"] != \"n/a\"][\n",
    "    \"split\"\n",
    "]\n",
    "train_fraction = train_fraction[train_fraction == \"train\"].shape[0] / train_fraction.shape[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(non_splitted)\n",
    "train_size = int(non_splitted.shape[0] * train_fraction)\n",
    "validation_size = int((non_splitted.shape[0] - train_size) / 2)\n",
    "\n",
    "train_indices = non_splitted[:train_size]\n",
    "validation_indices = non_splitted[train_size : train_size + validation_size]\n",
    "test_indices = non_splitted[train_size + validation_size :]\n",
    "\n",
    "touche_argument_classification.loc[train_indices, \"split\"] = \"train\"\n",
    "touche_argument_classification.loc[test_indices, \"split\"] = \"test\"\n",
    "touche_argument_classification.loc[validation_indices, \"split\"] = \"validation\"\n",
    "\n",
    "touche_argument_classification.groupby([\"split\", \"label\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483915e4-9968-45f8-833e-6b92c46a295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in touche_argument_classification[\"split\"].unique():\n",
    "    touche_argument_classification.loc[\n",
    "        touche_argument_classification[\"split\"] == split, [\"label\", \"statement\"]\n",
    "    ].to_csv(\n",
    "        DATA_DIR / \"statements\" / \"classification\" / f\"touche-{split}.tsv\",\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc185a8-b573-467b-b51c-14e71e350df3",
   "metadata": {},
   "source": [
    "# Dataset for Relation Classification\n",
    "\n",
    "We build this dataset based on the original data from Bcause and Touche."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d7f9d-c02e-493b-bf06-76d9d565af93",
   "metadata": {},
   "source": [
    "## Bcause\n",
    "\n",
    "Again, we build only data for training and validation from BCause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0849b2b-2164-4e40-8e25-e402bde3670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcause_argument_relation = []\n",
    "map_id_to_text = {}\n",
    "\n",
    "for instance in tqdm(bcause):\n",
    "    map_id_to_text[instance[\"id\"]] = instance[\"text\"]  # Needed later for identifying the text\n",
    "\n",
    "    if not isinstance(instance[\"metadata\"].get(\"related_to\"), str):\n",
    "        # We avoid the positions to rebuild the dataset\n",
    "        continue\n",
    "\n",
    "    bcause_argument_relation.append(\n",
    "        {\n",
    "            \"debate\": instance[\"metadata\"][\"debate\"],\n",
    "            \"source\": instance[\"id\"],\n",
    "            \"target\": instance[\"metadata\"][\"related_to\"],\n",
    "            \"label\": f'__label__{instance[\"metadata\"][\"type\"]}',  # Add the __label__ prefix to keep it standard\n",
    "        }\n",
    "    )\n",
    "\n",
    "bcause_argument_relation = pd.DataFrame(bcause_argument_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc74b46-d104-4fbc-8946-d376db9da3a0",
   "metadata": {},
   "source": [
    "To build the negative relations, we take the attack/support type arguments within a debate and compare them against other positions of that same debate, thus generating \"hard\" negative samples, as they are somewhat related talking about the same topic. Another way to deal with this can be sampling against every other possible position of the whole dataset, but those are softer negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b160a6d-5997-4af3-b258-d8df3abddd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcause_norel = []\n",
    "\n",
    "for debate, debate_df in tqdm(bcause_argument_relation.groupby(\"debate\")):\n",
    "    positions = debate_df[\"target\"].unique().tolist()  # Positions are always target\n",
    "    for position in tqdm(positions):\n",
    "        for _, statement in debate_df[debate_df[\"target\"] == position].iterrows():\n",
    "            bcause_norel.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"debate\": debate,\n",
    "                        \"source\": statement[\"source\"],\n",
    "                        \"target\": norel_position,\n",
    "                        \"label\": \"__label__noRel\",  # Add the __label__ prefix to keep it standard\n",
    "                    }\n",
    "                    for norel_position in positions\n",
    "                    if norel_position != position\n",
    "                ]\n",
    "            )\n",
    "\n",
    "bcause_norel = (\n",
    "    pd.DataFrame(bcause_norel).groupby(\"debate\").sample(frac=0.002)\n",
    ")  # We subsample, otherwise is too much\n",
    "\n",
    "bcause_argument_relation = pd.concat([bcause_argument_relation, bcause_norel], ignore_index=True)\n",
    "\n",
    "bcause_argument_relation[\"split\"] = \"train\"\n",
    "\n",
    "val_index = bcause_argument_relation.sample(frac=0.5, random_state=42).index\n",
    "\n",
    "bcause_argument_relation.loc[val_index, \"split\"] = \"validation\"\n",
    "bcause_argument_relation.groupby([\"split\", \"label\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db995194-fbde-4267-899e-1e38ae23839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcause_argument_relation[\"source\"] = bcause_argument_relation[\"source\"].map(map_id_to_text)\n",
    "bcause_argument_relation[\"target\"] = bcause_argument_relation[\"target\"].map(map_id_to_text)\n",
    "\n",
    "bcause_argument_relation = bcause_argument_relation[~bcause_argument_relation.target.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e39924-6610-4437-afd6-7e1c472d8dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in bcause_argument_relation[\"split\"].unique():\n",
    "    bcause_argument_relation.loc[\n",
    "        bcause_argument_relation[\"split\"] == split, [\"label\", \"source\", \"target\"]\n",
    "    ].to_csv(\n",
    "        DATA_DIR / \"statements\" / \"relation\" / f\"bcause-{split}.tsv\",\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9828557-7336-4c8a-9a75-43db63095172",
   "metadata": {},
   "source": [
    "## Touche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c40c3f-7da9-4c9a-8627-c76f148cfb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "touche_argument_relation = []\n",
    "map_id_to_text = {}\n",
    "\n",
    "for instance in tqdm(touche):\n",
    "    map_id_to_text[instance[\"id\"]] = instance[\"text\"]  # Needed later for identifying the text\n",
    "\n",
    "    if not isinstance(instance[\"metadata\"].get(\"related_to\"), str):\n",
    "        # We avoid the positions to rebuild the dataset\n",
    "        continue\n",
    "\n",
    "    touche_argument_relation.append(\n",
    "        {\n",
    "            \"subdataset\": instance[\"metadata\"][\"subdataset\"],\n",
    "            \"source\": instance[\"id\"],\n",
    "            \"target\": instance[\"metadata\"][\"related_to\"],\n",
    "            \"label\": f'__label__{instance[\"metadata\"][\"type\"]}',  # Add the __label__ prefix to keep it standard\n",
    "            \"split\": instance[\"metadata\"][\"original_split\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "touche_argument_relation = pd.DataFrame(touche_argument_relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1744db0-2ee9-4a77-ada4-ec42ff611913",
   "metadata": {},
   "source": [
    "In this case we use the same subdataset, it isn't as hard as the case of Bcause, but it's still a better solution than going everything vs everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec84d5-7c8a-44e1-8db7-710019216c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "touche_norel = []\n",
    "\n",
    "for subdataset, subdataset_df in tqdm(touche_argument_relation.groupby(\"subdataset\")):\n",
    "    positions = subdataset_df[\"target\"].unique().tolist()  # Positions are always target\n",
    "    for position in tqdm(positions):\n",
    "        for _, statement in subdataset_df[subdataset_df[\"target\"] == position].iterrows():\n",
    "            touche_norel.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"subdataset\": subdataset,\n",
    "                        \"source\": statement[\"source\"],\n",
    "                        \"target\": norel_position,\n",
    "                        \"label\": \"__label__noRel\",  # Add the __label__ prefix to keep it standard\n",
    "                        \"split\": \"n/a\",\n",
    "                    }\n",
    "                    for norel_position in positions\n",
    "                    if norel_position != position\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa03fe-6fc8-46fd-be44-8c8287b2fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "touche_norel = (\n",
    "    pd.DataFrame(touche_norel).groupby(\"subdataset\").sample(frac=0.01)\n",
    ")  # We subsample, otherwise is too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d56b0d0-ef85-421f-9cdc-023ed2b5b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "touche_argument_relation = pd.concat([touche_argument_relation, touche_norel], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4532a7f-6253-46f4-ba3f-77f21d91ab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_splitted = touche_argument_relation[touche_argument_relation[\"split\"] == \"n/a\"].index.values\n",
    "\n",
    "train_fraction = touche_argument_relation[touche_argument_relation[\"split\"] != \"n/a\"][\"split\"]\n",
    "train_fraction = train_fraction[train_fraction == \"train\"].shape[0] / train_fraction.shape[0]\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(non_splitted)\n",
    "train_size = int(non_splitted.shape[0] * train_fraction)\n",
    "validation_size = int((non_splitted.shape[0] - train_size) / 2)\n",
    "\n",
    "train_indices = non_splitted[:train_size]\n",
    "validation_indices = non_splitted[train_size : train_size + validation_size]\n",
    "test_indices = non_splitted[train_size + validation_size :]\n",
    "\n",
    "touche_argument_relation.loc[train_indices, \"split\"] = \"train\"\n",
    "touche_argument_relation.loc[test_indices, \"split\"] = \"test\"\n",
    "touche_argument_relation.loc[validation_indices, \"split\"] = \"validation\"\n",
    "\n",
    "touche_argument_relation.groupby([\"split\", \"label\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8931c4-31fa-4726-b1a2-fe0ee5e19ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "touche_argument_relation[\"source\"] = touche_argument_relation[\"source\"].map(map_id_to_text)\n",
    "touche_argument_relation[\"target\"] = touche_argument_relation[\"target\"].map(map_id_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4cd86-cff8-4839-91ff-ff4841e79bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in touche_argument_relation[\"split\"].unique():\n",
    "    touche_argument_relation.loc[\n",
    "        touche_argument_relation[\"split\"] == split, [\"label\", \"source\", \"target\"]\n",
    "    ].to_csv(\n",
    "        DATA_DIR / \"statements\" / \"relation\" / f\"touche-{split}.tsv\",\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        sep=\"\\t\",\n",
    "        index=False,\n",
    "        header=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
