{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e859221-c6d0-4003-8429-e06f8fe1c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader averaged_perceptron_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f5936-cde2-45b2-aaec-ad81212a4250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import pos_tag, sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbaa73-42f0-4273-953e-aef546b82811",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../data/clean/TaejaeAcademy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30263f-6584-48f0-b176-0fc9b1bf8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = sorted(DATA_DIR.glob(\"*.json\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef9fbd-0352-45d3-8fdf-a7de208e35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, \"rt\") as fh:\n",
    "    data = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63127c85-53a9-4148-865d-dfc4634bdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [f\"{t['speaker'].upper()}: {t['text']}\" for t in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b81c1-f8c4-41a4-ac65-f0b0ea7d0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203d400-56c8-453b-b166-f8d3ce1af92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR / fname.name.replace(\".json\", \".conll\"), \"wt\") as fh:\n",
    "    for paragraph in texts:\n",
    "        idx = 1\n",
    "        for sentence in sent_tokenize(paragraph):\n",
    "            for token, tag in pos_tag(word_tokenize(sentence)):\n",
    "                print(f\"{idx}\\t{token}\\t{stemmer.stem(token)}\\t{tag}\\t_\\t_\\t_\", file=fh)\n",
    "                idx += 1\n",
    "            print(file=fh)\n",
    "        print(file=fh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
